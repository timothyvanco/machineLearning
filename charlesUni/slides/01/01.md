title: NPFL129, Lecture 1
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Introduction to Machine Learning

## Milan Straka

### October 07, 2019

---
section: Machine Learning
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $x∈ℝ$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, …
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, …)
    - _reinforcement learning_, _semi-supervised learning_, …
- Measure P
    - _accuracy_, _error rate_, _F-score_, …

---
# Deep Learning Highlights
- Image recognition

~~~ ~
# Deep Learning Highlights
![w=60%,h=center](imagenet_recognition.jpg)

~~~ ~~
- Object detection
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](object_detection.pdf)

~~~ ~~
- Image segmentation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](image_segmentation.pdf)

~~~ ~~
- Human pose estimation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](human_pose_estimation.pdf)

~~~ ~~
- Image labeling
~~~ ~
# Deep Learning Highlights
![w=75%,h=center](image_labeling.pdf)

~~~ ~~
- Visual question answering
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](vqa.pdf)

~~~ ~~
- Speech recognition and generation
~~~ ~
# Deep Learning Highlights

<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/washington_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/columbia_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/fox_question.wav"></audio>

![w=100%](tacotron_comparison.pdf)

~~~ ~~
- Lip reading
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](lrw_showcase.pdf)
![w=70%,h=center](lipnet_saliency.pdf)

~~~ ~~
- Machine translation
~~~ ~
# Deep Learning Highlights
![w=44%,h=center](attention_visualization.pdf)

~~~ ~~
- Machine translation without parallel data
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](umt_ideas.pdf)
![w=30%,h=center](umt_comparison.pdf)

~~~ ~~
- Chess, Go and Shogi
~~~ ~
# Deep Learning Highlights
![w=95%,h=center](a0_results.pdf)

~~~ ~~
- Multiplayer Capture the flag
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](ctf_overview.pdf)

~~~ ~~

---
class: wide
# Introduction to Machine Learning History

![w=99%,h=center](figure1_ANN_history.jpg)

---
# Machine and Representation Learning

![w=35%,h=center](machine_learning.pdf)

---
section: TL;DR
# Basic Machine Learning Settings

Assume we have an input of $→x ∈ ℝ^d$.

~~~
Then the two basic ML tasks are:
1. **regression**: The goal of a regression is to predict real-valued target
   variable $t ∈ ℝ$ of the given input.

~~~
2. **classification**: Assuming we have a fixed set of $K$ labels, the goal
   of a classification is to choose a corresponding label/class for a given
   input.
~~~
   - We can predict the class only.
~~~
   - We can predict the whole distribution of all classes probabilities.

~~~
We usually have a **training set**, which is assumed to consist of examples
of $(→x, t)$ generated independently from a **data generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data generating distribution.

---
# Notation

- $a$, $→a$, $⇉A$, $⇶A$: scalar (integer or real), vector, matrix, tensor

- $⁇a$, $⁇→a$, $⁇⇉A$: scalar, vector, matrix random variable

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{∂f}{∂x}$: partial derivative of $f$ with respect to $x$

~~~
- $∇_→x f$: gradient of $f$ with respect to $→x$, i.e.,
  $\left(\frac{∂f(→x)}{∂x_1}, \frac{∂f(→x)}{∂x_2}, \ldots, \frac{∂f(→x)}{∂x_n}\right)$

---
section: Linear Regression
# Example Dataset

Assume we have the following data, generated from an underlying curve
by adding a small amount of Gaussian noise.

![w=57%,h=center](sin_data.pdf)

---
# Linear Regression

Given an input value $→x ∈ ℝ^d$, one of the simplest models to predict
a target real value is **linear regression**:
$$f(→x; →w, b) = x_1 w_1 + x_2 w_2 + … + x_D w_D + b = ∑_{i=1}^d x_i w_i + b = →x^T →w + b.$$
The $→w$ are usually called _weights_ and $b$ is called _bias_.

~~~
Sometimes it is convenient not to deal with the bias separately. Instead,
we might enlarge the input vector $→x$ by padding a value 1, and consider only
$→x^T→w$, where the role of a bias is accomplished by the last weight.
Therefore, when we say “weights”, we usually mean both weights and biases.

---
# Separate Bias vs. Padding $⇉X$ with Ones

Using an explicit bias term in the form of $f(x) = →w^T →x + b$.
$$
\begin{bmatrix}
x_{11} \quad x_{12} \\
x_{21} \quad x_{22} \\
\vdots \\
x_{n1} \quad x_{n2} \\
\end{bmatrix} \cdot
\begin{bmatrix}
w_1 \\ w_2
\end{bmatrix} + b
=
\begin{bmatrix}
w_1 x_{11} + w_2 x_{12} + b \\
w_1 x_{21} + w_2 x_{22} + b \\
\vdots \\
w_1 x_{n1} + w_2 x_{n2} + b
\end{bmatrix}
$$

~~~
With extra $1$ padding in $⇉X$ and an additional $b$ weight representing the
bias.
$$
\begin{bmatrix}
x_{11} & x_{12} & 1 \\
x_{21} & x_{22} & 1 \\
& \vdots & \\
x_{n1} & x_{n2} & 1 \\
\end{bmatrix} \cdot
\begin{bmatrix}
w_1 \\ w_2 \\ b
\end{bmatrix}
=
\begin{bmatrix}
w_1 x_{11} + w_2 x_{12} + b \\
w_1 x_{21} + w_2 x_{22} + b \\
\vdots \\
w_1 x_{n1} + w_2 x_{n2} + b
\end{bmatrix}
$$

---
# Linear Regression

Assume we have a dataset of $N$ input values $→x_1, …, →x_N$ and targets
$t_1, …, t_N$.

To find the values of weights, we usually minimize an **error function**
between the real target values and their predictions.

~~~
A popular and simple error function is _mean squared error_:
![w=40%,f=right](mse.pdf)

$$\operatorname{MSE}(→w) = \frac{1}{N} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2.$$

~~~
Often, _sum of squares_
$$\frac{1}{2} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2$$
is used instead, because the math comes out nicer.

---
# Linear Regression

There are several ways how to minimize the error function, but in the case of
linear regression and sum of squares error, there exists an explicit solution.

Our goal is to minimize the following quantity:
$$\tfrac{1}{2}∑_i^N (→x_i^T→w - t_i)^2.$$

~~~
Note that if we denote $⇉X ∈ ℝ^{N×D}$ the matrix of input values with $→x_i$ on a row $i$
and $→t ∈ ℝ^N$ the vector of target values, we can rewrite the minimized
quantity as
$$\tfrac{1}{2}||⇉X→w - →t||^2.$$

---
# Linear Regression

In order to find a minimum of $\tfrac{1}{2}∑_i^N (→x_i^T→w - t_i)^2$,
we can inspect values where the derivative of the
error function is zero, with respect to all weights $w_j$.

~~~
$$\frac{∂}{∂w_j} \tfrac{1}{2}∑_i^N (→x_i^T→w - t_i)^2 = \tfrac{1}{2} ∑_i^N \left(2(→x_i^T→w - t_i) x_{ij}\right) = ∑_i^N x_{ij}(→x_i^T→w - t_i)$$

~~~
Therefore, we want for all $j$ that $∑_i^N x_{ij}(→x_i^T→w - t_i) = 0$. We can
write all the equations together using matrix notation as $⇉X^T(⇉X→w - →t) = 0$
and rewrite to
$$⇉X^T⇉X→w = ⇉X^T →t.$$

~~~
The matrix $⇉X^T⇉X$ is of size $D×D$. If it is regular, we can compute its
inverse and therefore
$$→w = (⇉X^T⇉X)^{-1}⇉X^T→t.$$

---
# Linear Regression
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$).<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of linear regression.

- $→w ← (⇉X^T⇉X)^{-1}⇉X^T→t.$
</div>

~~~
The algorithm has complexity $𝓞(ND^2)$, assuming $N≥D$.

~~~
When the matrix $⇉X^T⇉X$ is singular, we can solve $⇉X^T⇉X→w = ⇉X^T →t$ using
SVD, which will be demonstrated on the next lecture.

---
# Linear Regression Example

Assume our input vectors comprise of $→x = (x^0, x^1, …, x^M)$, for $M ≥ 0$.

![w=60%,h=center](sin_lr.pdf)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](sin_errors.pdf)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.pdf)

---
# Linear Regression Overfitting

Note that employing more data also usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.pdf)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_  but not necessarily
its training error).

~~~
$L_2$ regularization (also called weighted decay) penalizes models
with large weights:

$$\frac{1}{2} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2 + \frac{λ}{2} ||→w||^2$$

![w=60%,h=center](sin_regularization.pdf)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} ||⇉X→w - →t||^2 + \tfrac{λ}{2} ||→w||^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(⇉X^T⇉X + λ⇉I)→w = ⇉X^T→t,$$
where $⇉I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), constant $λ ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of regularized linear regression.

- $→w ← (⇉X^T⇉X + λ⇉I)^{-1}⇉X^T→t.$
</div>

---
# Choosing Hyperparameters

_Hyperparameters_ are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $λ$.

~~~
![w=45%,h=center](sin_regularization_ablation.pdf)
