title: NPFL129, Lecture 8
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Model Combinations, Gradient Boosted Trees, Naive Bayess

## Milan Straka

### December 09, 2019

---
section: Refresh
# Decision Trees

The idea of decision trees is to partition the input space into usually cuboid
regions and solving each region with a simpler model.

We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, …

![w=80%,mw=49%,h=center](../06/tree_partitioning.pdf)
![w=90%,mw=49%,h=center](../06/tree_representation.pdf)

---
# Regression Decision Trees

Assume we have an input dataset $⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$. At the beginning,
the decision tree is just a single node and all input examples belong to this
node. We denote $I_𝓣$ the set of training example indices belonging to a leaf
node $𝓣$.

For each leaf, our model will predict the average of the training examples
belonging to that leaf, $t̂_𝓣 = \frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} t_i$.

We will use a _criterion_ $c_𝓣$ telling us how _uniform_ or _homogeneous_ are the
training examples belonging to a leaf node $𝓣$ – for regression, we will
employ the sum of squares error between the examples belonging to the node and the predicted
value in that node; this is proportional to variance of the training examples belonging
to the leaf node $𝓣$, multiplied by the number of the examples. Note that even
if it not _mean_ squared error, it is sometimes denoted as MSE.
$$c_\textrm{SE}(𝓣) ≝ ∑_{i ∈ I_𝓣} (t_i - t̂_𝓣)^2\textrm{, where } t̂_𝓣=\frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} t_i.$$

---
# Tree Construction

To split a node, the goal is to find a feature and its value such that when
splitting a node $𝓣$ into $𝓣_L$ and $𝓣_R$, the resulting regions decrease the
overall criterion value the most, i.e., the difference $c_{𝓣_L} + c_{𝓣_R} - c_𝓣$
is the lowest.

Usually we have several constraints, we mention on the most common ones:
- **maximum tree depth**: we do not split nodes with this depth;
- **minimum examples to split**: we only split nodes with this many training
  examples;
- **maximum number of leaf nodes**

The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until some above
  constraint is invalidated;
- if the maximum number of leaf nodes is give, we usually split such leaf $𝓣$
  where the criterion difference $c_{𝓣_L} + c_{𝓣_R} - c_𝓣$ is the lowest.

---
# Classification Decision Trees

For multi-class classification, we predict such class most frequent
in the training examples belonging to a leaf $𝓣$.

To define the criterions, let us denote the average probability for class $k$ in
a region $𝓣$ at $p_{𝓣}(k)$.

For classification trees, one of the following two criterions is usually used:

- **Gini index**:
  $$c_\textrm{Gini}(𝓣) ≝ |I_𝓣| ∑_k p_𝓣(k) \big(1 - p_𝓣(k)\big)$$

- **Entropy Criterion**
  $$c_\textrm{entropy}(𝓣) ≝ |I_𝓣| H(p_𝓣) = - |I_𝓣| ∑_k p_𝓣(k) \log p_𝓣(k)$$

---
section: Gini and Entropy Losses
# Binary Gini as (M)SE Loss

Recall that $I_𝓣$ denotes the set of training example indices belonging to a leaf node $𝓣$,
let $n_𝓣(0)$ be the number of examples with target value 0, $n_𝓣(1)$ be the
number of examples with target value 1, and let $p_𝓣 = \frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} t_i$.

~~~
Consider sum of squares loss $𝓛(p) = ∑_{i ∈ I_𝓣} (p - t_i)^2$.

~~~
By setting the derivative of the loss to zero, we get that the $p$ minimizing
the loss fulfils $|I_𝓣|p = ∑_{i ∈ I_𝓣} t_i$, i.e., $p = p_𝓣$.

~~~
The value of the loss is then
$$\begin{aligned}
  𝓛(p_𝓣) &= ∑_{i ∈ I_𝓣} (p_𝓣 - t_i)^2 = n_𝓣(0) (p_𝓣 - 0)^2 + n_𝓣(1) (p_𝓣 - 1)^2 \\
         &= \frac{n_𝓣(0) n_𝓣(1)^2}{\big(n_𝓣(0) + n_𝓣(1)\big)^2} + \frac{n_𝓣(1) n_𝓣(0)^2}{\big(n_𝓣(0) + n_𝓣(1)\big)^2}
          = \frac{n_𝓣(0) n_𝓣(1)}{n_𝓣(0) + n_𝓣(1)} \\
         &= \big(n_𝓣(0) + n_𝓣(1)\big) (1-p_𝓣) p_𝓣 = |I_𝓣| p_𝓣 (1-p_𝓣)
\end{aligned}$$

---
# Entropy as NLL Loss

Again let $I_𝓣$ denote the set of training example indices belonging to a leaf node $𝓣$,
let $n_𝓣(c)$ be the number of examples with target value $c$, and let
$p_𝓣(c) = \frac{n_𝓣(c)}{|I_𝓣|} = \frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} [t_i = c]$.

~~~
Consider non-averaged NLL loss $𝓛(p) = ∑_{i ∈ I_𝓣} - \log p(t_i | →x_i)$.

By setting the derivative of the loss with respect to $p(c)$ to zero,
we get that the $p$ minimizing the loss fulfils $|I_𝓣|p(c) = n_𝓣(c)$, i.e., $p(c) = p_𝓣(c)$.

~~~
The value of the loss with respect to $p_𝓣$ is then
$$\begin{aligned}
  𝓛(p_𝓣) &= ∑_{i ∈ I_𝓣} - \log p(t_i | →x_i) \\
         &= - ∑_c n_𝓣(c) \log p_𝓣(c) \\
         &= - |I_𝓣| ∑_c p_𝓣(c) \log p_𝓣(c) = |I_𝓣| H(p_𝓣)
\end{aligned}$$

---
section: Ensembling
# Ensembling

Ensembling is combining several models with a goal of reaching higher
performance.

~~~
The simplest approach is to train several independent models and then averaging
their output.

~~~
Given that for independent identically distributed random values $X_i$ we have
$$\begin{aligned}
\Var(∑ X_i) &= ∑ \Var(X_i) \\
\Var(a ⋅ X) &= a^2 \Var(X),
\end{aligned}$$
we get that
$$\Var\left(\frac{1}{n}∑ X_i\right) = \frac{1}{n} \Var(X_1).$$
Therefore, if the models exhibit independent errors, these errors will cancel
out with more models.

---
# Bagging

For neural network models, the simple ensembling is usually enough, given that
the loss has many local minima, so the models tend to be quite independent just
when using different initialization.

~~~
However, algorithms with a convex loss functions usually converge to the same
optimum independent on randomization.

~~~
In these cases, we can use **bagging**, which stands for **bootstrap
aggregation**.

~~~
![w=50%,f=right](bagging.pdf)

In bagging, we construct a different dataset for every model to be trained.
We construct it using **bootstrapping** – we sample as many training instances
as the original dataset has, but **with replacement**.

Such dataset is sampled using the same empirical data distribution and has the
same size, but is not identical.

---
section: RF
# Random Forests

Bagging of data combined with random subset of features (sometimes
called _feature bagging_).

![w=80%,h=center](random_forest.pdf)

---
# Random Forests

## Random Subset of Features

During each node split, only a random subset of features is considered when
finding a best split. A fresh random subset is used for every node.

~~~
## Extra Trees

The so-called extra trees are even more randomized, not finding the best
possible feature value when choosing a split, but considering only boundaries
with a uniform distribution within a feature's empirical range (minimum and
maximum in the training data).

---
section: Gradient Boosing
# Gradient Boosting

$$y(→x_i) = ∑_k f_k(→x_i; →W_k)$$

![w=80%,h=center](gbt_example.pdf)

---
# Gradient Boosting

$$𝓛(⇉W) = ∑_i \ell\big(t_i, y(→x_i)\big) + ∑_k \frac{1}{2} λ ||→W_k||^2$$

~~~
$$𝓛^{(t)}(⇉W) = ∑_i \ell\big(t_i, y^{(t-1)} + f_t(→x_i)\big) + \frac{1}{2} λ ||→W_t||^2$$

~~~
The original idea was to set $f_t(→x_i) ≈ \frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)}$
as a direction minimizing the residual loss and then finding a suitable constant
$γ_t$ so that $∑_i \ell\big(t_i, y^{(t-1)} + γ_t f_t(→x_i)\big)$ is as small as
possible.

---
# Gradient Boosting

However, a more principled approach was suggested later.

Denoting
$$\begin{aligned}
 g_i &= \frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)} \\
 h_i &= \frac{∂^2\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)^2} \\
\end{aligned}$$

~~~
we can expand the objective $𝓛^{(t)}$ using a second-order approximation to
$$𝓛^{(t)}(⇉W) ≈ ∑_i \Big[\ell\big(t_i, y^{(t-1)}\big) + g_i f_t(→x_i) + \frac{1}{2} h_i f_t^2(→x_i)\Big] + \frac{1}{2} λ ||→W_t||^2.$$

---
# Gradient Boosting

We recall that we denote indices of instances belonging to a node $𝓣$ as $I_𝓣$,
and let us denote the prediction for the node $𝓣$ as $w_𝓣$. Then we can rewrite
$$\begin{aligned}
𝓛^{(t)}(⇉W) &≈ ∑_i g_i f_t(→x_i) + \frac{1}{2} h_i f_t^2(→x_i)\Big] + \frac{1}{2} λ ||→W_t||^2 + \textrm{const} \\
            &≈ ∑_𝓣 \Big[\big(∑_{i ∈ I_𝓣} g_i\big) w_𝓣 + \frac{1}{2} \big(λ + ∑_{i ∈ I_𝓣} h_i\big) w_𝓣^2\Big] + \textrm{const}\\
\end{aligned}$$

~~~
By setting a derivative with respect to $w_𝓣$ to zero, we get the optimal
weight for a node $𝓣$:
$$w^*_𝓣 = -\frac{∑_{i ∈ I_𝓣} g_i}{λ + ∑_{i ∈ I_𝓣} h_i}.$$

---
# Gradient Boosting

Substituting the optimum weights to the loss, we get
$$𝓛^{(t)}(⇉W) ≈ -\frac{1}{2} ∑_𝓣 \frac{\left(∑_{i ∈ I_𝓣} g_i\right)^2}{λ + ∑_{i ∈ I_𝓣} h_i},$$

which can be used as a splitting criterion.

~~~
![w=60%,h=center](gbt_scores.pdf)

---
# Gradient Boosting

![w=70%,h=center](gbt_algorithm.pdf)

---
# Gradient Boosting

Furthermore, gradient boosted trees frequently use:
- data subsampling: either bagging, or use only a fraction of the original training;
~~~
- feature bagging;
~~~
- shrinkage: multiply each trained tree by a learning rate $α$, which reduces
  influence of each individual tree and leaves space for future optimization.

~~~
## Implementations

There are several efficient implementations, capable of distributed processing
of data larger than available memory:
- XGBoost
- LightGBM

~~~
## Playground

You can explore the [Gradient Boosted Trees playground](https://ufal.mff.cuni.cz/~straka/courses/npfl129/1920/slides/08/gbt/).
