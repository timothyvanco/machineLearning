title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax,<br> Support Vector Machines

## Milan Straka

### November 18, 2019

---
section: Refresh
# Lagrange Multipliers – Equality Constraints

![w=28%,f=right](../04/lagrange_equalities.pdf)

Given a funtion $J(→x)$, we can find a maximum with respect to a vector
$→x ∈ ℝ^d$, by investigating the critical points $∇_→x J(→x) = 0$.

Consider now finding maximum subject to a a constraing $g(→x) = 0$.

- Note that $∇_→x g(→x)$ is orthogonal to the surface of the constraing, because
  if $→x$ and a nearby point $→x+→ε$ lie on the surface, from the Taylor
  expansion $g(→x+→ε) ≈ g(→x) + →ε^T ∇_→x g(→x)$ we get $→ε^T ∇_→x g(→x) ≈ 0$.

- In the seeked maximum, $∇_→x f(→x)$ must also be orthogonal to the constraing
  surface (or else moving in the direction of the derivative would increase the
  value).

- Therefore, there must exist $λ$ such that $∇_→x f + λ∇_→x g = 0$.

---
# Lagrange Multipliers – Equality Constraints

![w=28%,f=right](../04/lagrange_equalities.pdf)

We therefore introduce the _Lagrangian function_
$$L(→x, λ) ≝ f(→x) + λg(→x).$$

We can then find the maximum under the constraing by inspecting critical points
of $L(→x, λ)$ with respect to both $→x$ and $λ$:
- $\frac{∂L}{∂λ} = 0$ leads to $g(→x)=0$;
- $\frac{∂L}{∂→x} = 0$ is the previously derived $∇_→x f + λ∇_→x g = 0$.

---
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(→w)$ with
respect to a vector $→w ∈ ℝ^d$, by investigating the critical points $∇_→w J(→w) = 0$.

A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[⋅]$.

Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(→x)$ for all points
$→x$. The functional derivative of $J$ with respect to a function $f$ in a point
$→x$ is denoted as
$$\frac{∂}{∂f(→x)} J.$$

For this class, we will use only the following theorem, which states that for
all differentiable functions $f$ and differentiable functions $g(f(→x), →x)$ with
continuous derivatives, it holds that
$$\frac{∂}{∂f(→x)} ∫g(f(→x), →x) \d→x = \frac{∂}{∂y} g(y, →x).$$

---
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(→x)$ as a vector of uncountably many
elements (for every value $→x)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $→w ∈ ℝ^d$:
$$\frac{∂}{∂w_i} ∑_j g(w_j, →x) = \frac{∂}{∂w_i} g(w_i, →x).$$
$$\frac{∂}{∂f(→x)} ∫g(f(→x), →x) \d→x = \frac{∂}{∂y} g(y, →x).$$

---
class: dbend
# Continuous Distribution with Maximum Entropy

What distribution over $ℝ$ maximizes entropy $H[p] = -𝔼_x \log p(x)$?

For continuous values, the entropy is an integral $H[p] = -∫p(x) \log p(x) \d x$.

We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution – we need to add
  a constraint that $∫p(x) \d x=1$;
~~~
- the problem is unspecified because a distribution can be shifted without
  changing entropy – we add a constraing $𝔼[x] = μ$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $σ^2$ has maximum entropy – adding a constraing
  $\Var(x) = σ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraings and the entropy function is
$$L(p; μ, σ^2) = λ_1 \Big(∫p(x) \d x - 1\Big) + λ_2 \big(𝔼[x] - μ\big) + λ_3\big(\Var(x) - σ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; μ, σ^2) =& ∫\Big(λ_1 p(x) + λ_2 p(x) x λ_3 p(x) (x - μ)^2 - p(x)\log p(x) \Big) \d x - \\
              & -λ_1 - μ λ_2 - σ^2λ_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{∂}{∂p(x)} L(p; μ, σ^2) = λ_1 + λ_2 x + λ_3 (x - μ)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:
$$\frac{∂}{∂p(x)} L(p; μ, σ^2) = λ_1 + λ_2 x + λ_3 (x - μ)^2 - 1 - \log p(x) = 0.$$
we obtain
$$p(x) = \exp\Big(λ_1 + λ_2 x + λ_3 (x-μ)^2 - 1\Big).$$

~~~
We can verify that setting $λ_1 = 1 - \log σ \sqrt{2π}$, $λ_2=0$ and $λ_3=-1/(2σ^2)$
fulfils all the constraints, arriving at
$$p(x) = 𝓝(x; μ, σ^2).$$

---
section: SoftMax Derivation
class: dbend
# Derivation of Softmax using Maximum Entropy

Let $𝕏 = \{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$ be training data
of a $K$-class classification, with $→x_i ∈ ℝ^D$ and $t_i ∈ \{1, 2, …, K\}$.

~~~
We want to model it using a function $π: ℝ^D → ℝ^K$
so that $π(→x)$ gives a distribution of classes for input $→x$.

~~~
We impose the following conditions on $π$:
- $$π(→x)_j ≥ 0$$
~~~
- $$∑_{j=1}^K π(→x)_j = 1$$
~~~
- $$∀_{k ∈ \{1, 2, …, D\}}, ∀_{j ∈ \{1, 2, …, K\}}: ∑_{i=1}^N π(→x_i)_j x_{i,k} = ∑_{i=1}^N \Big[t_i == j\Big] x_{i,k}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

There are many such $π$, one particularly bad is
$$π(→x) = \begin{cases}
  t_i&\textrm{if there exists }i: →x_i = →x, \\
  0&\textrm{otherwise}.\end{cases}$$

~~~
Therefore, we want to find a more general $π$ – we will aim for one with maximum
entropy.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We therefore want to minimize $-∑_{i=1}^N ∑_{j=1}^K π(→x_i)_j \log(π(→x_i)_j)$
given
- $π(→x)_j ≥ 0$,
- $∑_{i=j}^K π(→x)_j = 1$,
- $∀_{k ∈ \{1, …, D\}}, ∀_{j ∈ \{1, …, K\}}: ∑_{i=1}^N π(→x_i)_j x_{i,k} = ∑_{i=1}^N \big[t_i == j\big] x_{i,k}$.

~~~
We therefore form a Lagrangian
$$\begin{aligned}
L =& ∑_{k=1}^D ∑_{j=1}^K λ_{k,j} \Big(∑_{i=1}^N π(→x_i)_j x_{i,k} - \big[t_i == j\big] x_{i,k}\Big)\\
   & -∑_{i=1}^N β_i \Big(∑_{j=1}^K π(→x_i)_j - 1\Big) \\
   & -∑_{i=1}^N ∑_{j=1}^K π(→x_i)_j \log(π(→x_i)_j)
\end{aligned}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We now compute partial derivatives of the Lagrangian, notably the values
$$\frac{∂}{∂π(→x_i)_j}L.$$

~~~
We arrive at
$$\frac{∂}{∂π(→x_i)_j}L = →λ_{*,j} →x_i + β_i - \log(π(→x_i)_j) - 1.$$

~~~
Setting the Lagrangian to zero, we get $→λ_{*,j} →x_i + β_i - \log(π(→x_i)_j) - 1 = 0,$
which we rewrite to
$$π(→x_i)_j = e^{→λ_{*,j}→x_i +β_i-1}.$$

~~~
Such a forms guarantees $π(→x_i)_j > 0$, which we did not include in the
conditions.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

In order to find out the $β_i$ values, we turn to the constraint
$$∑_j π(→x_i)_j = ∑_j e^{→λ_{*,j}→x_i +β_i-1} = 1,$$
from which we get
$$e^{β_i} = \frac{1}{∑_j e^{→λ_{*,j}→x_i-1}},$$

~~~
yielding
$$π(→x_i)_j = \frac{e^{→λ_{*,j}→x_i}}{∑_k e^{→λ_{*,k}→x_i}}.$$

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$φ(→x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ … \\ x_1^2 \\ x_1x_2 \\ … \\ x_2x_1 \\ … \\ x_1^3 \\ x_1^2x_2 \\ … \end{bmatrix}.$$

~~~
The SGD update for linear regression is then
$$→w ← →w + α\big(t - →w^T φ(→x)\big) φ(→x).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $𝓞(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $→w$ as a linear combination
of the input features $φ(→x_i)$.

~~~
By induction, $→w = 0 = ∑_i 0 ⋅ φ(→x_i)$, and assuming $→w = ∑_i β_i ⋅ φ(→x_i)$,
after a SGD update we get
$$\begin{aligned}
→w ←& →w + α∑_i \big(t_i - →w^T φ(→x_i)\big) φ(→x_i)\\
   =& ∑_i \Big(β_i + α \big(t_i - →w^T φ(→x_i)\big)\Big) φ(→x_i).
\end{aligned}$$

~~~
A individual update is $β_i ← β_i + α\Big(t_i - →w^T φ(→x_i)\Big)$, and
substituting for $→w$ we get
$$β_i ← β_i + α\Big(t_i - ∑\nolimits_j β_j φ(→x_j)^T φ(→x_i)\Big).$$

---
# Kernel Linear Regression

We can formulate the alternative linear regression algorithm (it would be called
a _dual formulation_):

<div class="algorithm">

**Input**: Dataset ($⇉X = \{→x_1, →x_2, …, →x_N\} ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$.<br>

- Set $β_i ← 0$
- Compute all values $K(→x_i, →x_j) = φ(→x_i)^T φ(→x_j)$
- Repeat until convergence
  - Update the coordinates, either according to a full gradient update:
    - $→β ← →β + α(→t-K→β)$
  - or alternatively use single-batch SGD, arriving at:
    - for $i$ in random permutation of $\{1, …, N\}$:
      - $β_i ← β_i + α\Big(t_i - ∑\nolimits_j β_j K(→x_i, →x_j)\Big)$

    In vector notation, we can write $→β ← →β + α(→t-K→β)$.
</div>

~~~
The predictions are then performed by computing $y(→x) = →w^T φ(→x) = ∑_i β_i →φ(→x_i)^T →φ(→x)$.

---
section: Kernels
# Kernel Trick

A single SGD update of all $β_i$ then takes $𝓞(N^2)$, given that we can
evaluate scalar dot product of $φ(→x_j)^T φ(→x_i)$ quickly.

~~~
$$\begin{aligned}
φ(→x)^T φ(→z) =& 1 + ∑_i x_i z_i + ∑_{i,j} x_i x_j z_i z_j + ∑_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              =& 1 + ∑_i x_i z_i + \Big(∑_i x_i z_i\Big)^2 + \Big(∑_i x_i z_i\Big)^3 \\
              =& 1 + →x^T →z + \big(→x^T →z\big)^2 + \big(→x^T →z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $φ$ as a function
$$K(→x, →z) ≝ φ(→x)^t φ(→z).$$

~~~
There is quite a lot of theory behind kernel construction. The most often used
kernels are:

~~~
- polynomial kernel or degree $d$
  $$K(→x, →z) = (γ →x^T→z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to $d$
  input features;
~~~
- Gaussian (or RBF) kernel
  $$K(→x, →z) = e^{-γ||→x-→z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  in a sense a combination of polynomial kernels of all degrees).

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed; we now consider
finding the one with _maximum margin_.

![w=100%,h=center](svm_margin.pdf)

---
# Support Vector Machines

Assume we have a dataset $⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, 1\}^N$, feature map $φ$ and model
$$y(→x) ≝ →φ(→x)^T →w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.pdf)

We already know that the distance of a point $→x_i$ to the decision boundary is
$$\frac{|y(→x_i)|}{||→w||} = \frac{t_i y(→x_i)}{||→w||}.$$

~~~
We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||→w||} \min_i \big[t_i (→φ(→x)^T →w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $→w$ and $b$ by a constant, we can
say that for the points closest to the decision boundary, it will hold that
$$t_i y(→x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(→x_i) ≥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||→w||} \min_i \big[t_i (→φ(→x)^T →w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1.$$

---
section: KKT
# Lagrange Multipliers – Inequality Constraints

Given a funtion $J(→x)$, we can find a maximum with respect to a vector
$→x ∈ ℝ^d$, by investigating the critical points $∇_→x J(→x) = 0$.

We even know how to incorporate constraints of form $g(→x) = 0$.

~~~
![w=25%,f=right](lagrange_inequalities.pdf)

We now describe how to include inequality constraints $g(→x) ≥ 0$.

~~~
The optimum can either be attained for $g(→x) > 0$, when the constraint is said
to be _inactive_, or for $g(→x) = 0$, when the constraint is sayd to be
_active_.

~~~
In the inactive case, the maximum is again a critical point of the Langrangian,
with $λ=0$.
~~~
When maximum is on boundary, it corresponds to a critical point
with $λ≠0$ – but note that this time the sign of the multiplier matters, because
maximum is attained only when gradient of $f(→x)$ is oriented away from the region
$g(→x) ≥ 0$. We therefore require $∇f(→x) = - λ∇g(→x)$ for $λ>0$.

~~~
In both cases, $λ g(→x) = 0$.

---
section: KKT
# Karush-Khun-Tucker Conditions

![w=25%,f=right](lagrange_inequalities.pdf)

Therefore, the solution to a maximization problem of $f(x)$ subject to $g(→x)≥0$
can be found by inspecting all points where the derivation of the Lagrangian is zero,
subject to the following conditions:
$$\begin{aligned}
g(→x) &≥ 0 \\
λ &≥ 0 \\
λ g(→x) &= 0
\end{aligned}$$

~~~
# Necessary and Sufficient KKT Conditions

The above conditions are necessary conditions for a minimum. However, it can be
proven that in the following settings, the conditions are also **sufficient**:
- if the objective to optimize is a _convex_ function,
~~~
- the equality constraings are continuously differentiable convex functions,
~~~
- the inequality constraints are affine functions.


---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1,$$
~~~
we write the Lagrangian with multipliers $→a=(a_1, …, a_N)$ as
$$L = \frac{1}{2} ||→w||^2 - ∑_i a_i \big[t_i y(→x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $→w$ and $b$ to zero, we get
$$\begin{aligned}
→w =& ∑_i a_i t_iφ(→x_i) \\
 0 =& ∑_i a_i t_i \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = ∑_i a_i -  \frac{1}{2} ∑_i ∑_j a_i a_j t_i t_j K(→x_i, →x_j)$$
with respect to the constraints $∀_i: a_i ≥ 0$, $∑_i a_i t_i = 0$
and kernel $K(→x, →z) = φ(→x)^T φ(→z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &≥ 0 \\
t_i y(→x_i) - 1 &≥ 0 \\
a_i \big(t_i y(→x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $→x$ are given by $y(→x) = ∑ a_i t_i K(→x, →x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.pdf)
