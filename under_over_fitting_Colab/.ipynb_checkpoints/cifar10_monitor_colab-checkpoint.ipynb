{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import BaseLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor(BaseLogger):\n",
    "    def __init__(self, figPath, jsonPath=None, startAt=0): # one argument followed by 2 optional ones\n",
    "        # store output path for figure, path to JSON serialized file, and starting epoch\n",
    "        super(TrainingMonitor, self).__init__()\n",
    "        self.figPath = figPath      # path to the output plot that we can use to visualize loss and acc over time\n",
    "        self.jsonPath = jsonPath    # path to serialize loss and acc values as JSON file - for training history\n",
    "        self.statAt = startAt       # starting epoch that training is resumed at when using ctrl+c\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history = {}           # initialize the history dictionary\n",
    "\n",
    "        # if JSON history path exists, load training history\n",
    "        if self.jsonPath is not None:\n",
    "            if os.path.exists(self.jsonPath):\n",
    "                self.history = json.loads(open(self.jsonPath).read())\n",
    "\n",
    "                # check to see if a starting epoch was supplied\n",
    "                if self.startAt > 0:\n",
    "                    # loop over the entries in the history log and trim any entries that are past the starting epoch\n",
    "                    for key in self.history.keys():\n",
    "                        self.history[key] = self.history[key][:self.statAt]\n",
    "\n",
    "    # epoch - integer representing epoch number, logs - dictionary contains training and validation loss+acc for epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # loop over the logs and update the loss, acc... for the entire training process\n",
    "        for (k, v) in logs.items():\n",
    "            l = self.history.get(k, [])\n",
    "            l.append(float(v))\n",
    "            self.history[k] = l\n",
    "\n",
    "        # check to see if training history should be serialized to file\n",
    "        if self.jsonPath is not None:\n",
    "            f = open(self.jsonPath, \"w\")\n",
    "            f.write(json.dumps(self.history))\n",
    "            f.close()\n",
    "\n",
    "        # ensure at least two epochs have passed before plotting (epoch starts at zero)\n",
    "        if len(self.history[\"loss\"]) > 1:\n",
    "            # plot the training loss and acc\n",
    "            N = np.arange(0, len(self.history[\"loss\"]))\n",
    "            plt.style.use(\"ggplot\")\n",
    "            plt.figure()\n",
    "            plt.plot(N, self.history[\"loss\"], label=\"train_loss\")\n",
    "            plt.plot(N, self.history[\"val_loss\"], label=\"val_loss\")\n",
    "            plt.plot(N, self.history[\"acc\"], label=\"train_acc\")\n",
    "            plt.plot(N, self.history[\"val_acc\"], label=\"val_acc\")\n",
    "            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(len(self.history[\"loss\"])))\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Loss/Acc\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVGGNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize model along with input shape to be \"channeÃ´s last\"\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        channelDimension = -1                           # -1 = last ordering\n",
    "\n",
    "        # if \"channels first\" update input shape and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            channelDimension = 1                        # batch normalization operatesover channels - in order to apply\n",
    "                                                        # BN need to know which axis to normalize over, 1 = first order\n",
    "\n",
    "\n",
    "        # first layer - (CONV => RELU => BN) * 2 => POOL => DO\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape = inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=channelDimension))\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape = inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=channelDimension))\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))    # node from POOL layer will be randomly disconnected from next layer with prob 25%\n",
    "\n",
    "        # second layer - (CONV => RELU => BN) * 2 => POOL => DO\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=channelDimension))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=channelDimension))\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "        # FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))                   # 512 nodes\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))                 # increasing probability to 50%\n",
    "\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] process ID: {}\".format(os.getpid()))\n",
    "\n",
    "# load training and testing data, scale it into the range [0, 1]\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\") / 255.0\n",
    "testX = testX.astype(\"float\") / 255.0\n",
    "\n",
    "# convert labels from integers to vectors\n",
    "labBin = LabelBinarizer()\n",
    "trainY = labBin.fit_transform(trainY)\n",
    "testY = labBin.transform(testY)\n",
    "\n",
    "# initialize label names for the CIFAR-10 dataset\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model = MiniVGGNet.build(width=32, height=32, depth=3, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# construct the set of callbacks\n",
    "figPath = os.path.sep.join([args[\"output\"], \"{}.png\".format(os.getpid())])\n",
    "jsonPath = os.path.sep.join([args[\"output\"], \"{}.json\".format(os.getpid())])\n",
    "callbacks = [TrainingMonitor(figPath, jsonPath = jsonPath)]\n",
    "\n",
    "# train network\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=64, epochs=100, callbacks=callbacks, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
