title: NPFL129, Lecture 3
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 21, 2019

---
section: Perceptron
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a _threshold_ and the classify an input as negative/positive
depending whether $y(→x) = →x^T→w + b$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because it
is symmetric and also because the _bias_ parameter acts as a trainable threshold
anyway.

---
# Binary Classification

![w=50%,f=right](binary_classification.pdf)

- Consider two points on the decision boundary. Because $y(→x_1)=y(→x_2)$,
  we have $(→x_1-→x_2)^T→w=0$, and so $→w$ is orthogonal to every vector on the
  decision surface – $→w$ is a _normal_ of the boundary.

~~~
- Consider $→x$ and let $→x_⊥$ be orthogonal projection of $x$ to the bounary,
  so we can write $→x=→x_⊥ + r\frac{→w}{||→w||}$. Multiplying both sides
  by $→w^T$ and adding $b$, we get that the distance of $→x$ to the boundary is
  $r=\frac{y(→x)}{||→w||}$.

~~~
- The distance of the decision boundary from origin is therefore
  $\frac{|b|}{||→w||}$.


---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t ∈ \{-1, +1\}$, the goal is to find weights $→w$ such that
for all train data
$$\operatorname{sign}(→w^T →x_i) = t_i,$$
or equivalently
$$t_i →w^T →x_i > 0.$$

~~~

![w=20%,f=right](linearly_separable.pdf)

Note that a set is called **linearly separable**, if there exist
a weight vector $→w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblat in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, +1\}$).<br>
**Output**: Weights $→w ∈ ℝ^D$ such that $t_i →x_i^T→w > 0$ for all $i$.

- $→w ← 0$
- until all examples are classified correctly, process example $i$:
  - $y ← →w^T→x_i$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $→w$ if the training set is linearly separable.

---
# Perceptron as SGD

Consider the main part of the perceptron algorithm:

<div class="algorithm">

  - $y ← →w^T→x_i$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We can derive the algorithm using on-line gradient descent, using
the following loss function
$$L(f(→x; →w), t) ≝ \begin{cases} -t →x^T →w & \textrm{if~}t →x^T →w ≤ 0 \\ 0 & \textrm{otherwise}\end{cases}
  = \max(0, -t→x^T →w) = \ReLU(-t→x^T →w).$$

~~~
In this specific case, the value of the learning rate does not actually matter,
because multiplying $→w$ by a constant does not change a prediction.

---
# Perceptron Example

![w=52%,h=center](perceptron_convergence.pdf)

---
class: dbend
# Proof of Perceptron Convergence

Let $→w_*$ be some weights separating the training data and let $→w_k$ be the
weights after $k$ non-trivial updates of the perceptron algorithm, with $→w_0$
being 0.

~~~
We will prove that the angle $α$ between $→w_*$ and $→w_k$ decreases at each step.
Note that
$$\cos(α) = \frac{→w_*^T →w_k}{||→w_*||⋅||→w_k||}.$$

---
class: dbend
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $||→x||$ is bounded by $R$,
and that $γ$ is the minimum margin of $→w_*$, so $t →w_*^T →x ≥ γ.$

~~~
First consider the dot product of $→w_*$ and $→w_k$:
$$→w_*^T →w_k = →w_*^T (→w_{k-1} + t_k →x_k) ≥ →w_*^T →w_{k-1} + γ.$$
By iteratively applying this equation, we get
$$→w_*^T →w_k ≥ kγ.$$

~~~
Now consider the length of $→w_k$:
$$\begin{aligned}
||→w_k||^2 &= ||→w_{k-1} + t_k→x_k||^2 = ||→w_{k-1}||^2 + 2t→w_{k-1}^T→x_k + ||→x_k||^2
\end{aligned}$$

~~~
Because $→x_k$ was misclassified, we know that $t →w_{k-1}^T →x_k < 0$, so
$||→w_k||^2 ≤ ||→w_{k-1}||^2 + R^2.$

---
class: dbend
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(α) = \frac{→w_*^T →w_k}{||→w_*||⋅||→w_k||} ≥ \frac{kγ}{\sqrt{kR^2}||→w_*||}.$$

~~~
Therefore, the $\cos(α)$ increases during every update. Because the value of
$\cos(α)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 ≤ \frac{kγ}{\sqrt{kR^2}||→w_*||}\textrm{~or~}k ≥ \frac{R^2||→w_*||^2}{γ^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.
~~~
- The algorithm cannot be easily extended to classification into more than two
  classes.
~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.

---
section: ProbabilityBasics
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $φ ∈ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
![w=50%,f=right](bernoulli_variance.pdf)

$$\begin{aligned}
  P(x) &= φ^x (1-φ)^{1-x} \\
  𝔼[x] &= φ \\
  \Var(x) &= φ(1-φ)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $→p ∈ [0, 1]^k$ such that $∑_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(→x) &= ∏\nolimits_i^k p_i^{x_i} \\
  𝔼[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have _additive_ information.

~~~
$$I(x) ≝ -\log P(x) = \log \frac{1}{P(x)}$$

---
# Information Theory

## Entropy

Amount of _surprise_ in the whole distribution.
$$H(P) ≝ 𝔼_{⁇x∼P}[I(x)] = -𝔼_{⁇x∼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

~~~ ~~
![w=40%,f=right](entropy_example.pdf)

- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

~~~
Note that in the continuous case, the continuous entropy (also called
_differential entropy_) as a bit different semantics, for example, it can be
negative.

~~~
From now on, all logarithms are _natural logarithms_ with base $e$.

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ≝ -𝔼_{⁇x∼P}[\log Q(x)]$$

~~~
- Gibbs inequality
    - $H(P, Q) ≥ H(P)$
    - $H(P) = H(P, Q) ⇔ P = Q$
~~~
    - Proof: We use that $\log x ≤ (x-1)$, with equality only for $x=1$.
~~~
      $$∑_x P(x) \log \frac{Q(x)}{P(x)} ≤ ∑_x P(x) \left(\frac{Q(x)}{P(x)}-1\right) = ∑_x Q(x) - ∑_x P(x) = 0.$$
~~~
    - Alternative proof: Using Jensen's inequality, we get
      $$∑_x P(x) \log \frac{Q(x)}{P(x)} ≤ \log ∑_x P(x) \frac{Q(x)}{P(x)} = \log ∑_x Q(x) = 0.$$

---
# Information Theory

## Corollary of the Gibbs inequality

For a categorical distribution with $n$ outcomes, $H(P) ≤ \log n$, because for
$Q(x) = 1/n$ we get $H(P) ≤ H(P, Q) = -∑_x P(x) \log Q(x) = \log n.$

~~~
## Nonsymmetry

Note that generally $H(P, Q) ≠ H(Q, P)$.

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D_\textrm{KL}(P || Q) ≝ H(P, Q) - H(P) = 𝔼_{⁇x∼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) ≥ 0$
- generally $D_\textrm{KL}(P || Q) ≠ D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $μ$ and variance $σ^2$:
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right)$$

For standard values $μ=0$ and $σ^2=1$ we get $𝓝(x; 0, 1) = \sqrt{\frac{1}{2π}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with _maximal entropy_
is exactly the normal distribution.

---
section: MLE
# Maximum Likelihood Estimation

Let $𝕏 = \{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $p̂_\textrm{data}$.

~~~
Let $p_\textrm{model}(t | →x; →w)$ be a family of distributions.

~~~
The *maximum likelihood estimation* of $→w$ is:

$$\begin{aligned}
→w_\mathrm{ML} &= \argmax_→w p_\textrm{model}(\mathbb X; →w) \\
               &= \argmax_→w ∏\nolimits_{i=1}^N p_\textrm{model}(t_i | →x_i; →w) \\
               &= \argmin_→w ∑\nolimits_{i=1}^N -\log p_\textrm{model}(t_i | →x_i; →w) \\
               &= \argmin_→w 𝔼_{⁇→x ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(t | →x; →w)] \\
               &= \argmin_→w H(p̂_\textrm{data}, p_\textrm{model}(→x; →w)) \\
               &= \argmin_→w D_\textrm{KL}(p̂_\textrm{data}||p_\textrm{model}(→x; →w)) \color{gray} + H(p̂_\textrm{data})
\end{aligned}$$

---
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model
family $p_\textrm{model}(⋅; →w)$, and assume there exists a unique
$→w_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(⋅; →w_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $→w_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $→w_m$ converges in probability to
  $→w_{p_\textrm{data}}$.

  Formally, for any $ε > 0$, $P(||→w_m - →w_{p_\textrm{data}}|| > ε) → 0$
  as $m → ∞$.

~~~
- MLE is in a sense most _statistic efficient_. For any consistent estimator, we
  might consider the average distance of $→w_m$ and $→w_{p_\textrm{data}}$,
  formally $𝔼_{⁇→x_1, \ldots, ⁇→x_m ∼ p_\textrm{data}} [||→w_m - →w_{p_\textrm{data}}||^2]$.
  It can be shown (Rao 1945, Cramér 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
section: LogisticRegression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  P(C_1 | →x) &= σ(→x^t →w + →b) \\
  P(C_0 | →x) &= 1 - P(C_1 | →x),
\end{aligned}$$
where $σ$ is a _sigmoid function_
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$σ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$σ'(x) = σ(x) \big(1 - σ(x)\big)$$

~~~
![w=100%](sigmoid.pdf)

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$P(C_1 | →x) = σ(f(→x; →w)) = \frac{1}{1 + e^{-f(→x; →w)}}$$
~~~
we can arrive at
$$f(→x; →w) = \log\left(\frac{P(C_1 | →x)}{P(C_0 | →x)}\right),$$
where the prediction of the model $f(→x; →w)$ is called a _logit_
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(→x; →w) = →x^T →w$, we use MLE (the maximum likelihood
estimation). Note that $P(C_1 | →x; →w) = σ(y(→x; →w))$.

~~~
Therefore, the loss for a batch $𝕏=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
𝓛(𝕏) = \frac{1}{N} ∑_i -\log(P(C_{t_i} | →x_i; →w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(P(C_{t_i} | →x_i; →w)$
  - $→w ← →w - α→g$
</div>

---
section: MulticlassLogisticRegression
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- Generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$y(→x; ⇉W)_i = →W_i →x.$$

~~~
- Generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(→z)_i = \frac{e^{z_i}}{∑_j e^{z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$σ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as _multinomial logistic regression_,
_maximum entropy classifier_ or _softmax regression_.

---
# Multiclass Logistic Regression

Note that as defined, the multiclass logistic regression is overparametrized.
It is possible to generate only $K-1$ outputs and define $z_K = 0$, which is
the approach used in binary logistic regression.

~~~
In this settings, analogously to binary logistic regression, we can recover the
interpretation of the model outputs $→y(→x; ⇉W)$ (i.e., the $\softmax$ inputs)
as _logits_:

$$y(→x; ⇉W)_i = \log\left(\frac{P(C_i | →x; →w)}{P(C_K | →x; →w)}\right).$$

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$P(C_i | →x; ⇉W) = \softmax(⇉W_i →x)i = \frac{e^{⇉W_i →x}}{∑_j e^{⇉W_j →x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, 1, …, K-1\}$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(P(C_{t_i} | →x_i; →w)$
  - $→w ← →w - α→g$
</div>

---
# Multiclass Logistic Regression

Note that the decision regions of the multiclass logistic regression are singly
connected and convex.

![w=50%,h=center](classification_convex_regions.pdf)
