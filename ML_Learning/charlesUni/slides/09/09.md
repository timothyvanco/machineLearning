title: NPFL129, Lecture 9
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Naive Bayes, K-Means, Gaussian Mixture

## Milan Straka

### December 16, 2019

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.pdf)

The idea of SVM for regression is to use an $ε$-insensitive error function
$$𝓛_ε\big(t, y(→x)\big) = \max\big(0, |y(→x) - t| - ε\big).$$

~~~
The primary formulation of the loss is then
$$C ∑_i 𝓛_ε\big(t, y(→x)\big) + \frac{1}{2} ||→w||^2.$$

~~~
![w=25%,f=right](svr.pdf)

In the dual formulation, we ideally require every example to be withing $ε$ of
its target, but introduce two slack variables $→ξ^-$, $→ξ^+$ to allow outliers. We therefore
minimize the loss
$$C ∑_i (ξ_i^- + ξ_i^+) + \tfrac{1}{2} ||→w||^2$$
while requiring for every example $t_i - ε - ξ_i^- ≤ y(→x) ≤ t_i + ε + ξ_i^+$ for $ξ_i^- ≥ 0, ξ_i^+ ≥ 0$.

---
# SVM For Regression

The Langrangian after substituting for $→w$, $b$, $→ξ^-$ and $→ξ^+$ we get
that we want to minimize
$$L = ∑_i (a_i^+ - a_i^-) t_i - ε ∑_i (a_i^+ + a_i^-)
      - \frac{1}{2} ∑_i ∑_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(→x_i, →x_j)$$

![w=40%,f=right](svr_example.pdf)

subject to
$$0 ≤ a_i^+, a_i^- ≤ C.$$

~~~
The prediction is then given by
$$y(→z) = ∑_i (a_i^+ - a_j^-) K(→z, →x_i) + b.$$

---
section: TF-IDF
# Term Frequency – Inverse Document Frequency

To represent a document, we might consider it a **bag of words**, and create
a feature space with a dimension of every word. We might represent the words as:

- **binary indicators**: 1/0 depending on whether a word is present in
  a document or not;
~~~
- **term frequency TF**: relative frequency of the term in the document;
  $$\mathit{TF}(t) = \frac{\textrm{number of occurrences of $t$ in the document}}{\textrm{number of terms in the document}}$$
~~~
- **inverse document frequency IDF**: we might represent the term using its
  self-information, where terms with lower probability have higher weights;
  $$\mathit{IDF}(t) = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing term $t$}\big[\textrm{optionally} + 1]}$$
~~~
- **TF-IDF**: product of $\mathit{TF}(t)$ and $\mathit{IDF}(t)$.

---
section: NaiveBayes
# Naive Bayes Classifier

Consider a discriminative classifier modelling probabilities
$$p(C_k|→x) = p(C_k | x_1, x_2, …, x_D).$$

~~~
We might use Bayes theorem and rewrite it to
$$p(C_k|→x) = \frac{p(C_k) p(→x | C_k)}{p(→x)}.$$

~~~
The so-called **Naive Bayes** classifier assumes all $x_i$
are independent given $C_k$, so we can write
$$p(→x | C_k) = p(x_1 | C_k) p(x_2 | C_k, x_1) p(x_3 | C_k, x_1, x_2) ⋯ p(x_D | C_k, x_1, …)$$
as
$$p(C_k | →x) ∝ p(C_k) ∏_i p(x_i | C_k).$$

---
# Naive Bayes Classifier

There are several used naive Bayes classifiers, depending on the distribution
$p(x_i | C_k)$:
~~~
- **Gaussian NB**: the probability $p(x_i | C_k)$ is modelled as a normal
  distribution $𝓝(μ_{i, k}, σ_{i, k}^2)$;
~~~
- **Multinomial NB**: the probability $p(x_i | C_k)$ is proportional to
  $p_{i, k}^{x_i}$, so the
  $$\log p(C_k, →x) = \log p(C_k) + ∑_i\log p_{i, k}^{x_i} = \log p(C_k) + ∑_i x_i \log p_{i, k} = b + →x^T →w$$
  is a linear model in the log space with $b = \log p(C_k)$ and $w_i = \log p_{i, k}$.
~~~
  Denoting $n_{i, k}$ as the sum of features $x_i$ for a class $C_k$, the
  probabilities $p_{i, k}$ are usually estimated as
  $$p_{i, k} = \frac{n_{i, k} + α}{∑_j n_{j, k} + αD}$$
  where $α$ is a _smoothing_ parameter accounting for terms not appearing in any
  document of class $C_k$.

---
# Naive Bayes Classifier

- **Bernoulli NB**: when the input features are binary, the $p(x_i | C_k)$ might
  also be a Bernoulli distribution
  $$p(x_i | C_k) = p_{i, k}^{x_i} ⋅ (1 - p_{i, k})^{(1-x_i)}.$$
~~~
  Similarly to multinomial NB, the probabilities are usually estimated as
  $$p_{i, k} = \frac{\textrm{number of documents of class $k$ with nonzero feature $i$} + α}{\textrm{number of documents of class $k$} + 2α}.$$
~~~
  The difference with respect to Multinomial NB is that Bernoulli NB explicitly
  models also an _absence of terms_.

~~~
Given that a Multinomial/Bernoulli NB fits $p(C_k, →x)$ as a linear model and
a logistic regression fits $p(C_k | →x)$ as a log-linear model, naive Bayes and 
logistic regression form a so-called _generative-discriminative_ pair, where
the naive Bayes is a _generative_ model, while logistic regression is
a _discriminative_ model.

---
section: MultivariateGaussian
# Multivariate Gaussian Distribution

Recall that
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right).$$

~~~
For $D$-dimensional vector $→x$, the multivariate Gaussian distribution takes
the form
$$𝓝(→x | →μ, ⇉Σ) ≝ \frac{1}{\sqrt{(2π)^D |Σ|}} \exp \left(-\frac{1}{2}(→x-→μ)^T ⇉Σ^{-1} (→x-→μ) \right).$$

---
# Multivariate Gaussian Distribution

The $⇉Σ$ is a _covariance_ matrix, and it is symmetrical. If we represent it
using its _eigenvectors_ $→u_i$ and _eigenvalues $λ_i$, we get
$$⇉Σ^{-1} = ∑_i \frac{1}{λ_i} →u_i →u_i^T,$$
~~~
from which we can see that the constant surfaces of the multivariate Gaussian
distribution are ellipsoids centered at $→μ$, with axes oriented at $→u_i$
with scaling factors $λ_i ^{1/2}$.

![w=30%](multivariate_gaussian_elipsoids.pdf)![w=60%](multivariate_gaussian_covariance.pdf)

---
section: Clustering
# Clustering

Clustering is an unsupervised machine learning technique, which given input
data tries to divide them into some number of groups, or _clusters_.

~~~
The number of clusters might be given in advance, or should also be inferred.

~~~
When clustering documents, we usually use TF-IDF normalized so that each
feature vector has length 1 (i.e., L2 normalization).

---
section: KMeans
# K-Means Clustering

Let $→x_1, →x_2, …, →x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $→x_i ∈ ℝ^D$. Let $K$, the number of target clusters,
be given.

~~~
Let each cluster be specified by a point $→μ_1, …, →μ_K$.
~~~
Further, let $z_{i, k} ∈ \{0, 1\}$ be a binary indicator variables describing whether input
example $→x_i$ is assigned to cluster $k$, and let each cluster be specified by
a point $→μ_1, …, →μ_K$, usually called the cluster _center_.

~~~
Our objective function $J$ which we aim to minimize is
$$J = ∑_{i=1}^N ∑_{k=1}^K z_{i, k} ||→x_i - →μ_k||^2.$$

---
# K-Means Clustering

To find out the cluster centers $→μ_i$ and input example assignments $z_{i, k}$,
we use the following iterative algorithm (which could be considered a coordinate
descent):
~~~
1. compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
   is achieved by
   $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin_j ||→x_i - →μ_j||^2 \\
                             0 & \textrm{~~otherwise}.\end{cases}$$

2. compute best possible $→μ_k = \argmin\nolimits_→μ ∑_i z_{i,k} ||→x_i-→μ||^2$.
~~~
   By computing a derivative with respect to $→μ$, we get
   $$→μ_k = \frac{∑_i z_{i,k} →x_i}{∑_i z_{i,k}}.$$

---
# K-Means Clustering

![w=55%,h=center](kmeans_example.pdf)

---
# K-Means Clustering

![w=60%,f=right](kmeans_convergence.pdf)

It is easy to see that:
- updating the cluster assignment $z_{i, k}$ decreases the loss $J$ or keeps it the same;
~~~
- updating the cluster centers again decreases the loss $J$ or keeps it the
  same.

~~~
K-Means clustering therefore converges to a local optimum. However, it
is quite sensitive to the starting initialization:
~~~
- It is common practise to run K-Means algorithm multiple times with different
  initialization and use the result with lowest $J$ (scikit-learn uses
  `n_init=10` by default).
~~~
- There exist better initialization schemes, a frequently used one is
  `k-means++`, where the first cluster center is chosen randomly and others are
  chosen proportionally to the square of their distance to the nearest cluster
  center.

---
# K-Means Clustering

![w=75%,h=center](kmeans_color_reduction.pdf)

---
section: GaussianMixture
# Gaussian Mixture

Let $→x_1, →x_2, …, →x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $→x_i ∈ ℝ^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussian in the form
$$p(→x) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k).$$
Therefore, each cluster is parametrized as $𝓝(→x | →μ_k, ⇉Σ_k)$.

~~~
Let $→z ∈ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = π_k.$$

~~~
Therefore, $p(→z) = ∏_k π_k^{z_k}$.

---
# Gaussian Mixture

We can write
$$p(→x) = ∑_{→z} p(→z) p(→x | →z) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k)$$
~~~
and the probability of the whole clustering is therefore
$$\log p(⇉X | →π, →μ, →Σ) = ∑_{i=1}^N \log \left(∑_{k=1}^K π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)\right).$$

~~~
To fit a Gaussian mixture model, we start with maximum likelihood estimation and
minimize
$$𝓛(⇉X) = - ∑_i \log ∑_{k=1}^K π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)$$

---
# Gaussian Mixture

The derivative of the loss with respect to $→μ_k$ gives
$$\frac{∂𝓛(⇉X)}{∂→μ_k} = - ∑_i \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)} ⇉Σ_k^{-1} \big(→x_i - →μ_k\big)$$

~~~
Denoting $r(z_{i,k}) = \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)}$,
setting the derivative equal to zero and multiplying by $⇉Σ_k^{-1}$, we get
$$→μ_k = \frac{∑_i r(z_{i,k}) →x_i}{∑_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | →x_i)$. Note that the responsibilities depend on
$→μ_k$, so the above equation is not an analytical solution for $→μ_k$, but
can be used as an _iterative_ algorithm for converting to local optimum.

---
# Gaussian Mixture

For $⇉Σ_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute derivative with respect a matrix, and also we
need to differentiate matrix determinant) and results in an analogous equation
$$→Σ_k = \frac{∑_i r(z_{i,k}) (→x_i - →μ_k) (→x_i - →μ_k)^T}{∑_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $→π$, we need to include the constraint
$∑_k π_k = 1$, so we form a Lagrangian $𝓛(⇉X) + λ\left(∑\nolimits_k π_k - 1\right)$,
and get
$$0 = ∑_i \frac{𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)} + λ,$$
~~~
from which we get $π_k ∝ ∑_i r(z_{i,k})$ and therefore
$$π_k = 1/N ⋅ ∑\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

![w=40%,h=center](mog_algorithm1.pdf)
![w=40%,h=center](mog_algorithm2.pdf)

---
# Gaussian Mixture

![w=75%,h=center](mog_data.pdf)
![w=75%,h=center](mog_illustration.pdf)

---
# Gaussian Mixture

![w=75%,h=center](mog_example.pdf)
