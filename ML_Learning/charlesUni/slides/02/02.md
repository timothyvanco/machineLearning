title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD, Perceptron

## Milan Straka

### October 14, 2019

---
section: Regression
# Linear Regression

Given an input value $→x ∈ ℝ^d$, one of the simplest models to predict
a target real value is **linear regression**:
$$f(→x; →w, b) = x_1 w_1 + x_2 w_2 + … + x_D w_D + b = ∑_{i=1}^d x_i w_i + b = →x^T →w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $→w$ if convenient.

~~~
By computing derivatives of a sum of squares error function, we arrived
at the following equation for the optimum weights:
$$⇉X^T⇉X→w = ⇉X^T →t.$$

~~~
If $⇉X^T ⇉X$ is regular, we can invert it and compute the weights as $→w = (⇉X^T⇉X)^{-1}⇉X^T→t$.

~~~
Matrix $⇉X^T ⇉X$ is regular if and only if $⇉X$ has rank $d$, which is
equivalent to the columns of $⇉X$ being linearly independent.

---
class: dbend
# SVD Solution of Linear Regression

Now consider the case that $⇉X^T ⇉X$ is singular. We will show that
$⇉X^T⇉X→w = ⇉X^T →t$ is still solvable, but it does not have a unique
solution. Our goal in this case will be to find the smallest $→w$ fulfilling the
equation.

~~~
We now consider _singular value decomposition (SVD)_ of X, writing $⇉X = ⇉U ⇉Σ ⇉V^T$,
where
- $⇉U ∈ ℝ^{N×N}$ is an orthogonal matrix, i.e., $→u_i^T →u_j = [i=j]$,
- $⇉Σ ∈ ℝ^{N×D}$ is a diagonal matrix,
- $⇉V ∈ ℝ^{D×D}$ is again an orthogonal matrix.

~~~
Assuming the diagonal matrix $⇉Σ$ has rank $r$, we can write it as
$$⇉Σ = \begin{bmatrix} ⇉Σ_r & ⇉0 \\ ⇉0 & ⇉0 \end{bmatrix},$$
where $⇉Σ_r∈ℝ^{d×d}$ is a regular diagonal matrix.
~~~
Denoting $⇉U_r$ and $⇉V_r$ the matrix of first $r$ columns of $⇉U$ and $⇉V$, respectively,
we can write $⇉X = ⇉U_r ⇉Σ_r ⇉V_r^T$.

---
class: dbend
# SVD Solution of Linear Regression

Using the decomposition $⇉X = ⇉U_r ⇉Σ_r ⇉V_r^T$, we can rewrite the goal equation
as
$$⇉V_r ⇉Σ_r^T ⇉U_r^T ⇉U_r ⇉Σ_r ⇉V_r^T →w = ⇉V_r ⇉Σ_r^T ⇉U_r^T →t.$$

~~~
A transposition of an orthogonal matrix is its inverse. Therefore, our
submatrix $⇉U_r$ fulfils that $⇉U_r^T ⇉U_r = ⇉I$, because $⇉U_r^T ⇉U_r$
is a top left submatrix of $⇉U^T ⇉U$. Analogously, $⇉V_r^T ⇉V_r = ⇉I$.

~~~
We therefore simplify the goal equation to
$$⇉Σ_r ⇉Σ_r ⇉V_r^T →w = ⇉Σ_r ⇉U_r^T →t$$

~~~
Because the diagonal matrix $⇉Σ_r$ is regular, we can divide by it and obtain
$$⇉V_r^T →w = ⇉Σ_r^{-1} ⇉U_r^T →t.$$

---
class: dbend
# SVD Solution of Linear Regression

We have $⇉V_r^T →w = ⇉Σ_r^{-1} ⇉U_r^T →t$. If he original matrix $⇉X^T ⇉X$ was
regular, then $r=d$ and $⇉V_r$ is a square regular orthogonal matrix, in which case
$$→w = ⇉V_r ⇉Σ_r^{-1} ⇉U_r^T →t.$$

~~~
If we denote $⇉Σ^+ ∈ ℝ^{D×N}$ the diagonal matrix with $Σ_{i,i}^{-1}$ on
diagonal, we can rewrite to
$$→w = ⇉V ⇉Σ^+ ⇉U^T →t.$$

~~~
Now if $r < d$, $⇉V_r^T →w = →y$ is undetermined and has infinitely many
solutions. To find the one with smallest norm $||→w||$, consider the full
product $⇉V^T→w$. Because $⇉V$ is orthogonal, $||⇉V^T→w||=||→w||$, and it is
sufficient to find $→w$ with smallest $||⇉V^T→w||$.
~~~
We know that the first $r$ elements of $||⇉V^T→w||$ are fixed by the above equation
– the smallest $||⇉V^T→w||$ can be therefore obtained by setting the last $d-r$
elements to zero.
~~~
Finally, we note that $⇉Σ^+ ⇉U^T →t$ is exactly $⇉Σ_r^{-1} ⇉U_r^T →t$ padded
with $d-r$ zeros, obtaining the same solution $→w = ⇉V ⇉Σ^+ ⇉U^T →t$.

---
class: dbend
# SVD Solution of Linear Regression and Pseudoinverses

The solution to a linear regression with sum of squares error function is
tightly connected to matrix pseudoinverses. If a matrix $⇉X$ is singular or
rectangular, it does not have an exact inverse, and $⇉X→w=→b$ does not
have an exact solution. 

~~~
However, we can consider the so-called _Moore-Penrose pseudoinverse_
$$⇉X^+ ≝ ⇉V ⇉Σ^+ ⇉U^T$$
to be the closest approximation to an inverse, in the sense that we can find
the best solution (with smallest MSE) to the equation $⇉X→w=→b$ by setting $→w=⇉X^+ →b$.

~~~
Alternatively, we can define the pseudoinverse as
$$⇉X^+ = \argmin_{⇉Y∈ℝ^{D×N}} ||⇉X ⇉Y - ⇉I_N||_F = \argmin_{⇉Y∈ℝ^{N×D}} ||⇉Y ⇉X - ⇉I_D||_F$$
which can be verified to be the same as our SVD formula.

---
section: Random Variables
# Random Variables
A random variable $⁇x$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $⁇x ∼ P$ stands for a random variable $⁇x$ having a distribution $P$.

~~~
For discrete variables, the probability that $⁇x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(⁇x = x)$. All probabilities are non-negative and sum
of probabilities of all possible values of $⁇x$ is $∑_x P(⁇x=x) = 1$.

~~~
For continuous variables, the probability that the value of $⁇x$ lies in the interval
$[a, b]$ is given by $∫_a^b p(x)\d x$.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$𝔼_{⁇x ∼ P}[f(x)] ≝ ∑_x P(x)f(x)$$

For continuous variables it is computed as:
$$𝔼_{⁇x ∼ p}[f(x)] ≝ ∫_x p(x)f(x)\d x$$

~~~
If the random variable is obvious from context, we can write only $𝔼_P[x]$
of even $𝔼[x]$.

~~~
Expectation is linear, i.e.,
$$𝔼_⁇x [αf(x) + βg(x)] = α𝔼_⁇x [f(x)] + β𝔼_⁇x [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $μ = 𝔼[x]$.

$$\begin{aligned}
  \Var(x) &≝ 𝔼\left[\big(x - 𝔼[x]\big)^2\right]\textrm{, or more generally} \\
  \Var(f(x)) &≝ 𝔼\left[\big(f(x) - 𝔼[f(x)]\big)^2\right]
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = 𝔼\left[x^2 - 2x𝔼[x] + \big(𝔼[x]\big)^2\right] = 𝔼\left[x^2\right] - \big(𝔼[x]\big)^2,$$
because $𝔼\big[2x𝔼[x]\big] = 2(𝔼[x])^2$.

~~~
Variance is connected to $E[x^2]$, a _second moment_ of a random
variable – it is in fact a _centered_ second moment.

---
# Estimators and Bias

An _estimator_ is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
For example, we might estimate _mean_ of random variable by sampling a value
according to its probability distribution.

~~~
_Bias_ of an estimator is the difference of the expected value of the estimator
and the true value being estimated:
$$\textrm{bias} = 𝔼[\textrm{estimate}] - \textrm{true estimated value}.$$

~~~
If the bias is zero, we call the estimator _unbiased_, otherwise we call it
_biased_.

---
# Estimators and Bias

If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $⁇x_1,
\ldots, ⁇x_n$ independent and identically distributed random variables, we might
estimate mean and variance as
$$μ̂ = \frac{1}{n} ∑\nolimits_i x_i,~~~σ̂_2 = \frac{1}{n} ∑\nolimits_i (x_i - μ̂)^2.$$
~~~
Such estimate is biased, because $𝔼[σ̂^2] = (1 - \frac{1}{n})σ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have small variance – in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=50%,f=right](gradient_descent.pdf)

Assuming we are minimizing an error function
$$\argmin_→w E(→w),$$
we may use _gradient descent_:
$$→w ← →w - α∇_→wE(→w)$$

~~~
The constant $α$ is called a _learning rate_ and specifies the “length”
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

Consider an error function computed as an expectation over the dataset:
$$∇_→w E(→w) = ∇_→w 𝔼_{(→x, t)∼p̂_\textrm{data}} L\big(f(→x; →w), t\big).$$

~~~
- **(Regular) Gradient Descent**: We use all training data to compute $∇_→w E(→w)$
  exactly.

~~~
- **Online (or Stochastic) Gradient Descent**: We estimate $∇_→w E(→w)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$∇_→w E(→w) ≈ ∇_→w L\big(f(→x; →w), t\big)\textrm{~~for randomly chosen~~}(→x, t)\textrm{~~from~~}p̂_\textrm{data}.$$

~~~
- **Minibatch SGD**: The minibatch SGD is a trade-off between gradient descent
  and SGD – the expectation in $∇_→w E(→w)$ is estimated using $m$ random independent
  examples from the training data.

$$∇_→w E(→w) ≈ \frac{1}{m} ∑_{i=1}^m ∇_→w L\big(f(→x_i; →w), t_i\big)
               \textrm{~~for randomly chosen~~}(→x_i, t_i)\textrm{~~from~~}p̂_\textrm{data}.$$

---
# Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $α_i$, and using a noisy estimate $J(→w)$ of the real
gradient $∇_→w E(→w)$:
$$→w_{i+1} ← →w_i - α_i J(→w_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro algorithm, 1951) that if
the loss function $L$ is convex and continuous, then SGD converges to the unique
optimum almost surely if the sequence of learning rates $α_i$ fulfills the
following conditions:
$$α_i \rightarrow 0,~~~∑_i α_i = ∞,~~~∑_i α_i^2 < ∞.$$

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only. However, note that finding a global minimum of an arbitrary
function is _at least NP-hard_.

---
# Gradient Descent Convergence

Convex functions mentioned on a previous slide are such that for $x_1, x_2$
and real $0 ≤ t ≤ 1$,
$$f(tx_1 + (1-t)x_2) ≤ tf(x_1) + (1-t)f(x_2).$$

![w=90%,mw=50%,h=center](convex_2d.pdf)![w=68%,mw=50%,h=center](convex_3d.pdf)

~~~
A twice-differentiable function is convex iff its second derivative is always
non-negative.

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$ and $-\log x$.

---
# Gradient Descent of Linear Regression

For linear regression and sum of squares, using online gradient descent we can
update the weights as
$$→w ← →w - α∇_→wE(→w) = →w - α(→x^T→w-t)→x.$$

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ which hopefully minimize MSE of linear regression.

- $→w ← 0$
- repeat until convergence:
  - for $i = 1, \ldots, n$:
    - $→w ← →w - α(→x_i^T→w-t_i)→x_i.$
</div>

---
section: Features
# Features

Note that until now, we did not explicitly distinguished _input_ instance values and
instance _features_.

~~~
The _input_ instance values are usually the raw observations and are given.
However, we might extend them suitably before running a machine learning
algorithm, especially if the algorithm is linear or otherwise limited and
cannot represent arbitrary function.

~~~
We already saw this in the example from the previous lecture, where even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, …, x^M)$:
![w=35%,h=center](../01/sin_lr.pdf)

---
# Features

Generally, it would be best if we have machine learning algorithms processing
only the raw inputs. However, many algorithms are capable of representing
only a limited set of functions (for example linear ones), and in that case,
_feature engineering_ plays a major part in the final model performance.
Feature engineering is a process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, …, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 ∀i$ and of $x_i x_j ∀i≠j$.

~~~
- **categorical one-hot features**: Assume for example that a day in a week is
  represented on the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366. Using these integral values as
  input to linear regression makes little sense – instead it might be better
  to learn weights for individual days in a week or for individual dog breeds.
  We might therefore represent input classes by binary indicators for every
  class, giving rise to **one-hot** representation, where input integral
  value $1 ≤ v ≤ L$ is represented as $L$ binary values, which are all
  zero except for the $v$-th one, which is one.

---
section: CV
# Cross-Validation

We already talked about a **train set** and a **test set**. Given that the main
goal of machine learning is to perform well on unseen data, the test set must
not be used during training nor hyperparameter selection. Ideally, it is hidden
to us altogether.

~~~
Therefore, to evaluate a machine learning model (for example to select model
architecture, input features, or hyperparameter value), we normally need the
**validation** or a **development** set.

~~~
However, using a single development set might give us noisy results. To obtain
less noisy results (i.e., with smaller variance), we can use
**cross-validation**.

~~~
![w=48%,f=right](k-fold_cross_validation.pdf)

In cross-validation, we choose multiple validation sets from the training data,
and for every one, we train a model on the rest of the training data and
evaluate on the chosen validation sets. A commonly used strategy to choose
the validation sets is called **k-fold cross-validation**. Here the training set is partitioned
into $k$ subsets of approximately the same size, and each subset takes turn
to play a role of a validation set.

---
section: Perceptron
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a _threshold_ and the classify an input as negative/positive
depending whether $→x^T→w$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because it
is symmetric and also because the _bias_ parameter acts as a trainable threshold
anyway.

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t ∈ \{-1, +1\}$, the goal is to find weights $→w$ such that
for all train data
$$\operatorname{sign}(→w^T →x_i) = t_i,$$
or equivalently
$$t_i →w^T →x_i > 0.$$

~~~
Note that a set is called **linearly separable**, if there exist
a weight vector $→w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblat in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, +1\}$).<br>
**Output**: Weights $→w ∈ ℝ^D$ such that $t_i →x_i^T→w > 0$ for all $i$.

- $→w ← 0$
- until all examples are classified correctly:
  - for $i$ in $1, …, N$:
    - $y ← →w^T→x_i$
    - if $t_i y ≤ 0$ (incorrectly classified example):
      - $→w ← →w + t_i →x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $→w$ if the training set is linearly separable.

---
# Perceptron as SGD

Consider the main part of the perceptron algorithm:

<div class="algorithm">

  - $y ← →w^T→x_i$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We can derive the algorithm using on-line gradient descent, using
the following loss function
$$L(f(→x; →w), t) ≝ \begin{cases} -t →x^T →w & \textrm{if~}t →x^T →w ≤ 0 \\ 0 & \textrm{otherwise}\end{cases}
  = \max(0, -t→x^T →w) = \ReLU(-t→x^T →w).$$

~~~
In this specific case, the value of the learning rate does not actually matter,
because multiplying $→w$ by a constant does not change a prediction.
